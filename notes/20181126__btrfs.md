To diagnose problems with drives in a btrfs filesystem

- start scrub: `sudo btrfs scrub start <mount point>`and wait a long time (e.g. 7H for 4x4 TB drives)
- check scrub status after: `sudo btrfs scrub status <mount point>` to see the number of unrecoverable errors
- find affected files in `dmesg` messages: `dmesg | grep BTRFS | grep path`
- check drives health with `smartctl` (installed via `sudo apt install smartmontools`):
 - `sudo smartctl -t short <dev path>` or `sudo smartctl -t long <dev path>` to start short or long test, in the  background
 - `sudo smartctl -a <dev path>` or `sudo smartctl -x <dev path>` to get short or long report about drive and test outcomesTo change a drive in a RAID1 array in BTRFS
- if possible, attach the new drive without removing the defective one
- if needed, you can wipe filesystem informations from this new drive using `sudo wipefs -a <dev path of new drive>`
- use `replace`command, `sudo btrfs replace start <ID> <dev new> <mount point>` where `<ID>` is the btrfs number for the device to replace (can be obtained using `sudo btrfs device usage <mount point>` for example)
- **do not** use `btrfs device delete` to remove the problematic drive, as btrfs will try to reduplicate data elsewhere, it will take ages and may not succeed depending on the actual remaining space, and this is not interruptable (well unless you shut down the computer, which is a bad idea in general)
- re-balance data across the RAID volume using `sudo btrfs balance start <mount point>` (use `-dusage` option, to avoid a full balancing that can take a very long time) and use `sudo btrfs balance status <mount point>` to monitor it

Good source of information: https://btrfs.wiki.kernel.org/index.php/Using_Btrfs_with_Multiple_Devices#Using_add_and_delete
